{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88b66c8d",
   "metadata": {},
   "source": [
    "# Face recognition using LBPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b59bc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pywhatkit as kit\n",
    "import smtplib, getpass\n",
    "import glob\n",
    "import os\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0766c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for cropping face and converting into gray scale\n",
    "\n",
    "def img_crop_gray(model,img):\n",
    "    \n",
    "    f= model.detectMultiScale(img)\n",
    "    \n",
    "    if len(f)==0:\n",
    "        #print('face not found')\n",
    "        return None, None\n",
    "    #crop image\n",
    "    crop = img[f[0][1]: f[0][1]+f[0][2],f[0][0]:f[0][0]+f[0][3], : ]\n",
    "    \n",
    "    #convert all images to the same dimensions\n",
    "    crop= cv2.resize(crop, (200,200))\n",
    "    \n",
    "    #convert the image to gray scale to reduce dimensionality\n",
    "    crop= cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    return crop,f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74d9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting dataset\n",
    "\n",
    "cam=cv2.VideoCapture(0)\n",
    "model= cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "i=0\n",
    "while i<100:\n",
    "    ret, pic= cam.read()\n",
    "    face,f=img_crop_gray(model,pic)\n",
    "    \n",
    "    if f is None:\n",
    "        cv2.putText(pic,'face not found', (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        cv2.imshow('face recognition',pic)\n",
    "        if cv2.waitKey(20)==13:\n",
    "            break\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        #save it to disk\n",
    "        file_path= 'C:/Users/Anagha/Documents/mlops/cv/faces/taylor/'+str(i)+ '.jpg'  #selena\n",
    "        cv2.imwrite(file_path, face)\n",
    "        i=i+1\n",
    "        \n",
    "        #show bounding box around face\n",
    "        cv2.rectangle(pic,(f[0][0],f[0][1]),(f[0][0]+f[0][3],f[0][1]+f[0][3]),[0,255,0], 3)\n",
    "        \n",
    "        #showing live count\n",
    "        cv2.putText(face, str(i), (50,50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "        \n",
    "        #show the image\n",
    "        cv2.imshow('face recognition',pic)\n",
    "        cv2.imshow('crop', face)\n",
    "        if cv2.waitKey(20)==13:\n",
    "            break\n",
    "            \n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "cam.release()\n",
    "print('dataset collected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all image file names into a variable\n",
    "\n",
    "file_loc_tay = os.path.join( 'faces','taylor', '*.jpg')\n",
    "file_loc_sel= os.path.join( 'faces','selena', '*.jpg')\n",
    "\n",
    "files_tay = glob.glob(file_loc_tay)\n",
    "files_sel = glob.glob(file_loc_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6112ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "files=files_tay\n",
    "\n",
    "for i in files_sel:\n",
    "    files.append(i)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fa972",
   "metadata": {},
   "outputs": [],
   "source": [
    "images,names,labels=[],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a74d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in files:\n",
    "    p=cv2.imread(i, cv2.IMREAD_GRAYSCALE)\n",
    "    images.append(p)\n",
    "    l=i.split('\\\\')\n",
    "    names.append(l[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e0cb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94185b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in names:\n",
    "    if i=='taylor':\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "labels=np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model creation\n",
    "model_both=cv2.face_LBPHFaceRecognizer.create()\n",
    "model_both.train(images,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10609c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws():\n",
    "    # to launch instance\n",
    "    # enter your specific details(image-id, subnet,security groups,keyname etc)\n",
    "    os.system(\"aws ec2 run-instances --image-id <enter_here> --instance-type t2.micro --count 1 --subnet-id <enter_here> --tags Key=Face_recognition,Value=Myec2 --security-group-ids <enter_here> --key-name <enter_here>\")\n",
    "    print(\"launched\")\n",
    "    # to create volume\n",
    "    os.system(\"aws ec2 create-volume --availability-zone <enter_here> --size 5 --tag-specifications ResourceType=volume,Tags={Key=Face_recognition,Value=Myebs}\")\n",
    "    print(\"volume created\")\n",
    "    print(sp.getoutput(\"aws ec2 describe-instances --filters \\\"Name=tag:Face_recognition,Values=Myec2\\\" --query \\\"Reservations[].Instances[].[InstanceId,LaunchTime]\\\"\"))\n",
    "    print(sp.getoutput(\"aws ec2 describe-volumes --filters \\\"Name=tag:Face_recognition,Values=Myebs\\\" --query \\\"Volumes[].[VolumeId,CreateTime]\\\"\"))\n",
    "    i_id = input(\"enter instance id:\")\n",
    "    v_id = input(\"enter volume id:\")\n",
    "    # to attach volume\n",
    "    os.system(\"aws ec2 attach-volume --instance-id \"+i_id+\" --volume-id \"+v_id+\" --device /dev/xvdf\" )\n",
    "    print(\"attached\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd3543",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[(\"<enter_phone_number>\", \"<enter_mail_id>\", \"<enter_password>\", \"<enter_recipients_mail>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for sending whatsapp msg and email\n",
    "\n",
    "def send_email_msg(tup):\n",
    "    ph,sender,pw,recipient= tup\n",
    "    #whatsapp\n",
    "    kit.sendwhatmsg(f\"+91{ph}\", \"hey, a face has been detected!\", datetime.now().hour, (datetime.now().minute) +1)\n",
    "    print('whatsapp message sent')\n",
    "    #gmail\n",
    "    s= smtplib.SMTP('smtp.gmail.com', 587)\n",
    "    \n",
    "    #start TLS\n",
    "    s.starttls()\n",
    "    \n",
    "    #authentication\n",
    "    \n",
    "    #sender login\n",
    "    s.login(sender, pw)\n",
    "    \n",
    "    #message to be sent\n",
    "    msg= 'hey a face has been detected!'\n",
    "    \n",
    "    #sending mail to recipient\n",
    "    \n",
    "    s.sendmail(sender, recipient, msg)\n",
    "    \n",
    "    #terminate session\n",
    "    s.quit()\n",
    "    \n",
    "    print('email sent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba40057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final working- model for 2 persons\n",
    "det_msg=0\n",
    "det_aws=0\n",
    "cam=cv2.VideoCapture(0)\n",
    "haarcascade= cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "while True:\n",
    "    ret, pic= cam.read()\n",
    "    face,f=img_crop_gray(haarcascade,pic)\n",
    "    if f is None:\n",
    "        \n",
    "        cv2.putText(pic, \"No Face Found\", (220, 120) , cv2.FONT_HERSHEY_COMPLEX, 1, (0,0,255), 2)\n",
    "        cv2.imshow('Face Recognition', pic )\n",
    "        \n",
    "        if cv2.waitKey(20)==13:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        \n",
    "        result= model_both.predict(face)\n",
    "        #print(result)\n",
    "        \n",
    "        if result[1] < 500:\n",
    "            confidence = int( 100 * (1 - (result[1])/400) )\n",
    "    \n",
    "    \n",
    "        if confidence > 85:\n",
    "            #show bounding box around face\n",
    "            cv2.rectangle(pic,(f[0][0],f[0][1]),(f[0][0]+f[0][3],f[0][1]+f[0][3]),[0,255,0], 3)\n",
    "            if result[0]==0:\n",
    "                cv2.putText(pic, \"Hey taylor  confidence:\"+str(confidence), (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "                det_msg=1\n",
    "            if result[0]==1:\n",
    "                cv2.putText(pic, \"Hey selena  confidence:\"+str(confidence), (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)\n",
    "                det_aws=1\n",
    "            cv2.imshow('Face Recognition', pic )\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            cv2.putText(pic, \"who are you?\", (0, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0,255,0), 2)        \n",
    "            cv2.imshow('Face Recognition', pic )\n",
    "        \n",
    "        if cv2.waitKey(20)==13:\n",
    "            break\n",
    "    \n",
    "            \n",
    "cv2.destroyAllWindows()\n",
    "cam.release()\n",
    "print('done')\n",
    "\n",
    "if det_msg==1:\n",
    "    send_email_msg(l[0])\n",
    "if det_aws==1:\n",
    "    aws()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
